# import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# with open('All_Combine.pickle', 'rb') as handle:
#   All_Combine = pickle.load(handle)
All_Combine = pd.read_pickle('All_Combine.pickle')
Data = All_Combine['600000']


VariableList = ['Quality_RoE', 'Quality_RoC', 'Quality_RoA',
'Quality_AssetTurnover', 'Quality_Gross margin',
'Quality_EBITDA margin', 'Quality_Berry Ratio',
'Quality_Current ratio', 'Quality_Long-term debt / equity',
'Quality_Operating Profit margin',
'PriceMoMRev_TotRet1D', 'PriceMoMRev_TotRet5D',
'PriceMoMRev_TotRet21D', 'PriceMoMRev_TotRet252D',
'PriceMoMRev_TotRet12M-1M', 'PriceMoMRev_TotRet1260D',
'PriceMoMRev_Lottery', 'PriceMoMRev_vol_volume21D',
'PriceMoMRev_Price-to-52 week high', 'Tech_Realized vol, 1Y',
'Tech_Skewness, 1Y', 'Tech_MAcrossover, 15W-36W, 1Y',
'Tech_CAPM_beta, 5Y monthly',
'Tech_CAPM_idosyncratic_vol, 1Y daily', 'Tech_Kurtosis, 1Y',
'Tech_FloatTurnover, 12M', 'Tech_Log capitalization',
'Growth_EPSGrowth', 'Value_EBITDA/EV',
'Value_Sales/EV', 'Value_PE', 'Value_PS', 'Value_PCF', 'Value_PB', 'Quality_ZScore']
Var = Data[VariableList]
Target = Data['target_monthly']
Sign = pd.DataFrame([1]*len(VariableList),index=VariableList)
Sign.ix[['PriceMoMRev_Lottery','PriceMoMRev_vol_volume21D','Tech_Realized vol, 1Y',
      'Tech_Skewness, 1Y', 'Tech_CAPM_beta, 5Y monthly', 'Tech_CAPM_idosyncratic_vol, 1Y daily',
      'Tech_Kurtosis, 1Y', 'Tech_FloatTurnover, 12M', 'Value_PE', 'Value_PS', 'Value_PCF',
      'Value_PB']] = -1

# print Var.corrwith(Target)
Perform = pd.DataFrame(np.random.randn(len(VariableList), 5),index=VariableList,
                       columns=['PnL', 'IC', 'SR', 'Turnover', 'SerialCorr'])
Perform2 = pd.DataFrame(np.random.randn(len(VariableList), 3),index=VariableList,
                       columns=['Low', 'Med', 'High'])
for Iter in VariableList:
  AllVar = pd.DataFrame()
  Target = pd.DataFrame()
  Price = pd.DataFrame()
  for Iter2 in All_Combine.keys():
    AllVar[Iter2] = All_Combine[Iter2][Iter]
    Target[Iter2] = All_Combine[Iter2]['target_monthly']
    Price[Iter2] = All_Combine[Iter2]['Close']
    Target.fillna(0,inplace=True)
  Weight = AllVar.ix[0:21,].copy()
  PnL = AllVar.iloc[0:21,0].copy()
  Ret = PnL.copy()
  SrCorr = PnL.copy()
  IC = AllVar.iloc[0:21,0].copy()
  Capital = AllVar.iloc[0:21,0].copy()
  Capital.ix[0,] = 100
  LevLimit = 1
  PriceMatrix = np.zeros((21,3))
  for rowID in range(0,21):
    Index = np.argsort(AllVar.ix[rowID,Price.ix[rowID,].notnull()])
    SortIndex = AllVar.ix[rowID,Price.ix[rowID,].notnull()].index[Index]
    N = int(len(Index)/3)
    Weight.ix[rowID,] = 0
    Weight.ix[rowID,SortIndex[:N]] = -1
    Weight.ix[rowID,SortIndex[-N:]] = 1
    Weight.ix[rowID,] = Weight.ix[rowID,] * Sign.ix[Iter,0]
    PriceMatrix[rowID,0] = np.mean(Target.ix[rowID,SortIndex[:N]])
    PriceMatrix[rowID,1] = np.mean(Target.ix[rowID,SortIndex[N:-N]])
    PriceMatrix[rowID,2] = np.mean(Target.ix[rowID,SortIndex[-N:]])

    Adj = LevLimit*Capital.ix[rowID]/sum(abs(Price.ix[rowID,].fillna(0))*abs(Weight.ix[rowID,]))
    Weight.ix[rowID,] = Weight.ix[rowID,]*Adj
    PnL.ix[rowID] = sum(Weight.ix[rowID,]*Target.ix[rowID,])
    IC.ix[rowID] = np.sqrt(1)*pd.DataFrame({'A' : AllVar.ix[rowID,].values.flatten(),
      'B' : Target.ix[rowID,].values.flatten()}).corr(method='spearman').ix[0,1]
    Ret.ix[rowID] = PnL.ix[rowID]/Capital.ix[rowID,]
    if rowID < 20:
      Capital.ix[rowID+1] = Capital.ix[rowID] + PnL.ix[rowID]

  #
  # plt.interactive(False)
  # plt.plot(PnL.index, cumPnL, 'ro')
  # plt.title(Iter)
  # plt.show()


  # plt.interactive(False)
  # plt.plot(PriceMatrix.mean(0), 'ro')
  # plt.title(Iter)
  # plt.show()

  Perform.ix[Iter,'PnL'] = Capital.ix[-1]/100-1
  Perform.ix[Iter,'Turnover'] = abs(Weight.diff()).sum().sum()
  Perform.ix[Iter,'IC'] = np.mean(IC)
  Perform.ix[Iter,'SR'] = np.mean(Ret) / np.std(Ret)
  Perform.ix[Iter,'SerialCorr'] = np.sqrt(1)*pd.DataFrame({'A' : AllVar.ix[:-1,].values.flatten(),
      'B' : AllVar.ix[1:,].values.flatten()}).corr(method='spearman').ix[0,1]
  Perform2.ix[Iter,] = np.mean(PriceMatrix,axis=0)
print Perform
print Perform2

# Simple Linear Model where factors are norm inv rank values
import scipy.stats as ss
from scipy.stats import norm
RankedVar = {}
NormVar = {}
for Iter in VariableList:
  AllVar = pd.DataFrame()
  for Iter2 in All_Combine.keys():
    AllVar[Iter2] = All_Combine[Iter2][Iter]
  InvVar = AllVar.copy()
  for rowID in range(AllVar.shape[0]):
    AllVar.ix[rowID,] = ss.rankdata(AllVar.ix[rowID,])/46
    InvVar.ix[rowID,] = norm.ppf(AllVar.ix[rowID,])
  RankedVar[Iter] = AllVar
  NormVar[Iter] = AllVar

AllNormVar = pd.DataFrame()
for Iter in VariableList:
  AllNormVar[Iter] = np.asarray(NormVar[Iter].ix[0:21,].as_matrix()).flatten()

Target = pd.DataFrame()
Price = pd.DataFrame()
MCap = pd.DataFrame()
for Iter2 in All_Combine.keys():
  Target[Iter2] = All_Combine[Iter2]['target_monthly']
  Price[Iter2] = All_Combine[Iter2]['Close']
  MCap[Iter2] = All_Combine[Iter2]['MarketCap']
Target.fillna(0,inplace=True)
MCap.fillna(0,inplace=True)

# use LASSO and BIC for Var Selection
from sklearn.linear_model import Lasso, LassoCV, LassoLarsIC, LassoLarsCV, LassoLars,lasso_path, ElasticNetCV, ElasticNet
X = AllNormVar
Y = np.asarray(Target.ix[0:21,].as_matrix()).flatten()


model_bic = LassoLarsIC(criterion='bic')
model_bic.fit(X, Y)

model_cv = LassoCV()
model_cv.fit(X, Y)

model1 = Lasso(alpha=model_bic.alpha_)
model1.fit(X, Y)

model = Lasso(alpha=model_cv.alpha_)
model.fit(X, Y)

# clf = ExtraTreesClassifier()
# clf = clf.fit(X, Y)

Var1 = [val for (idx, val) in enumerate(VariableList) if np.abs(model.coef_[idx])>1e-10]
Var2 = [val for (idx, val) in enumerate(VariableList) if np.abs(model1.coef_[idx])>1e-10]
Var3 = [val for (idx, val) in enumerate(Perform.index) if np.abs(Perform.ix[val,'IC'])>0.02]

from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor, ExtraTreesClassifier
from sklearn.svm import SVR, NuSVR, NuSVC
from sklearn.grid_search import GridSearchCV, ParameterGrid
from sklearn.metrics import mean_squared_error,log_loss
from sklearn import cross_validation

# make forecast one month ahead and update model
Pf2 = []
Weight = AllVar.ix[21:,].copy()
Capital = AllVar.iloc[20:,0].copy()
Capital.ix[0,] = 100
PnL = AllVar.iloc[21:,0].copy()
Ret = PnL.copy()

VariableList1 = VariableList
VariableList1 = ['Quality_Berry Ratio',
'Quality_Current ratio', 'Quality_Long-term debt / equity',
'Quality_Operating Profit margin',
'PriceMoMRev_TotRet1D', 'PriceMoMRev_TotRet5D','PriceMoMRev_TotRet252D',
'PriceMoMRev_TotRet1260D', 'PriceMoMRev_Lottery',
'PriceMoMRev_Price-to-52 week high', 'Tech_Realized vol, 1Y',
'Tech_Skewness, 1Y', 'Tech_MAcrossover, 15W-36W, 1Y',
'Tech_CAPM_beta, 5Y monthly',
'Tech_CAPM_idosyncratic_vol, 1Y daily', 'Tech_Kurtosis, 1Y',
'Tech_FloatTurnover, 12M', 'Tech_Log capitalization',
'Growth_EPSGrowth', 'Value_EBITDA/EV',
'Value_Sales/EV', 'Value_PE', 'Value_PS', 'Value_PCF', 'Value_PB', 'Quality_ZScore']

SVMParam = [{'kernel': 'rbf', 'C': 0.0001, 'nu': 0.94000000000000006, 'gamma': 0.1},
{'kernel': 'rbf', 'C': 0.0001, 'nu': 0.93500000000000005, 'gamma': 100},
{'kernel': 'rbf', 'C': 0.0001, 'nu': 0.92000000000000004, 'gamma': 100},
{'kernel': 'rbf', 'C': 0.01, 'nu': 0.90000000000000002, 'gamma': 0.1},
{'kernel': 'sigmoid', 'C': 0.0001, 'nu': 0.90000000000000002, 'gamma': 0.0001},
{'kernel': 'sigmoid', 'C': 0.0001, 'nu': 0.92500000000000004, 'gamma': 0.1},
{'kernel': 'sigmoid', 'C': 0.1, 'nu': 0.97500000000000009, 'gamma': 0.001},
{'kernel': 'sigmoid', 'C': 0.0001, 'nu': 0.99500000000000011, 'gamma': 1},
{'kernel': 'rbf', 'C': 0.01, 'nu': 0.9900000000000001, 'gamma': 0.001},
{'kernel': 'sigmoid', 'C': 0.001, 'nu': 0.9900000000000001, 'gamma': 0.1}]

SVMParam1 = [{'kernel': 'linear', 'nu': 0.050000000000000003, 'gamma': 0.0012589254117941675},
{'kernel': 'linear', 'nu': 0.050000000000000003, 'gamma': 0.31622776601683955},
{'kernel': 'linear', 'nu': 0.050000000000000003, 'gamma': 0.79432823472428615},
{'kernel': 'linear', 'nu': 0.050000000000000003, 'gamma': 0.0039810717055349778},
{'kernel': 'linear', 'nu': 0.050000000000000003, 'gamma': 0.39810717055349937},
{'kernel': 'sigmoid', 'nu': 0.050000000000000003, 'gamma': 0.015848931924611172},
{'kernel': 'linear', 'nu': 0.050000000000000003, 'gamma': 0.002511886431509582}]

for rowID in range(21,31):
  # rank and norminv transform
  AllNormVar = pd.DataFrame()
  AllNormVarTest = pd.DataFrame()
  for Iter in VariableList1:
    AllNormVar[Iter] = np.asarray(NormVar[Iter].ix[0:rowID,].as_matrix()).flatten()
    AllNormVarTest[Iter] = np.asarray(NormVar[Iter].ix[rowID,].as_matrix()).flatten()
  X = AllNormVar
  Y = np.asarray(Target.ix[0:rowID,].as_matrix()).flatten()
  YTest = np.asarray(Target.ix[rowID,].as_matrix()).flatten()

  model_bic = LassoLarsIC(criterion='bic')
  model_bic.fit(X, Y)
  model1 = Lasso(alpha=model_bic.alpha_)
  model1.fit(X, Y)
  PredY0 = model1.predict(X)
  PredY = model1.predict(AllNormVarTest)
  BstPara = 0


  # model_cv = LassoCV(cv=5,alphas=model_bic.alphas_,random_state=0).fit(X,Y)
  # model1 = Lasso(alpha=model_cv.alpha_).fit(X,Y)
  # PredY0 = model1.predict(X)
  # PredY = model1.predict(AllNormVarTest)
  # BstPara = 0

  # model_cv = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],cv=5,random_state=0).fit(X,Y)
  # PredY0 = model_cv.predict(X)
  # PredY = model_cv.predict(AllNormVarTest)
  # BstPara = 0

  # ExtraTreeRegressor
  # parameters = list(ParameterGrid({'max_features': ('auto', 'log2'), 'max_depth': [1, 2, 3, 4, 5, 10, 20, 50, None]}))
  # # parameters = list(ParameterGrid({'max_features': ('auto', 'log2'), 'max_depth': [10, 20, 50, None]}))
  # k_fold = cross_validation.KFold(n=len(Y), n_folds=5)
  # BstPara = None
  # BstScore = 1e5
  # for para in parameters:
  #     Score = 0
  #     for train_indices, test_indices in k_fold:
  #         TrX = X.ix[train_indices,]; TrY = Y[train_indices]
  #         TeX = X.ix[test_indices,]; TeY = Y[test_indices]
  #         clf1 = ExtraTreesRegressor(random_state=0, max_depth=para['max_depth'],max_features=para['max_features'],n_jobs=5)
  #         clf1.fit(TrX,TrY)
  #         PeY = clf1.predict(TeX)
  #         Score += np.sqrt(mean_squared_error(TeY,PeY))
  #     if Score < BstScore:
  #         BstScore = Score
  #         BstPara = para
  # clf = ExtraTreesRegressor(random_state=0, max_depth=BstPara['max_depth'],max_features=BstPara['max_features'],n_jobs=5)
  # clf.fit(X,Y)
  # PredY0 = clf.predict(X)
  # PredY = clf.predict(AllNormVarTest)

  # RandomForestRegressor
  # parameters = list(ParameterGrid({'max_features': ('auto', 'log2'), 'max_depth': [1, 2, 3, 4, 5, 10, 20, 50, None],
  #                                  'n_estimators': [25, 50, 100, 200, 500, 700, 1000, 2000, 5000]}))
  # k_fold = cross_validation.KFold(n=len(Y), n_folds=5)
  # BstPara = None
  # BstScore = 1e5
  # for para in parameters:
  #     Score = 0
  #     for train_indices, test_indices in k_fold:
  #         TrX = X.ix[train_indices,]; TrY = Y[train_indices]
  #         TeX = X.ix[test_indices,]; TeY = Y[test_indices]
  #         clf1 = RandomForestRegressor(random_state=1, max_depth=para['max_depth'], max_features=para['max_features'],
  #                                      n_estimators=para['n_estimators'],n_jobs=4)
  #         clf1.fit(TrX,TrY)
  #         PeY = clf1.predict(TeX)
  #         Score += np.sqrt(mean_squared_error(TeY,PeY))
  #     if Score < BstScore:
  #         BstScore = Score
  #         BstPara = para
  # clf = RandomForestRegressor(random_state=1, max_depth=BstPara['max_depth'], max_features=BstPara['max_features'],
  #                             n_estimators=BstPara['n_estimators'],n_jobs=4)
  # clf.fit(X_train_minmax,Y)
  # PredY0 = clf.predict(X_train_minmax)
  # PredY = clf.predict(X_test_minmax)


  # GradientBoosting
  # clf = GradientBoostingRegressor(random_state=1)


  # SVR
  # from sklearn import preprocessing
  # min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))
  # X_train_minmax = min_max_scaler.fit_transform(X)
  # X_test_minmax = min_max_scaler.transform(AllNormVarTest)
  # # parameters = list(ParameterGrid({'nu': np.arange(0.85,1.001,0.005), 'kernel': ('rbf', 'linear','sigmoid'),
  # #                                  'C': [10**ii for ii in np.arange(-5,1,.1)], 'gamma': [10**ii for ii in np.arange(-4,2,0.1)]}))
  # # # parameters = list(ParameterGrid({'epsilon': [10**ii for ii in range(-4,4)], 'kernel': ('rbf', 'linear','sigmoid'),
  # # #                                  'C': [10**ii for ii in range(-4,4)], 'gamma': [10**ii for ii in range(-4,4)]}))
  # # k_fold = cross_validation.KFold(n=len(Y), n_folds=5,random_state=4)
  # # BstPara = None
  # # BstScore = 1e5
  # # for para in parameters:
  # #     Score = 0
  # #     for train_indices, test_indices in k_fold:
  # #         # TrX = X_train_minmax.ix[train_indices,]; TrY = Y[train_indices]
  # #         # TeX = X_train_minmax.ix[test_indices,]; TeY = Y[test_indices]
  # #         TrX = X_train_minmax[train_indices,]; TrY = Y[train_indices]
  # #         TeX = X_train_minmax[test_indices,]; TeY = Y[test_indices]
  # #         # clf1 = SVR(epsilon=para['epsilon'],kernel=para['kernel'],C=para['C'],gamma=para['gamma'])
  # #         clf1 = NuSVR(nu=para['nu'],kernel=para['kernel'],C=para['C'],gamma=para['gamma'])
  # #         clf1.fit(TrX,TrY)
  # #         PeY = clf1.predict(TeX)
  # #         Score += np.sqrt(mean_squared_error(TeY,PeY))
  # #     if Score < BstScore:
  # #         BstScore = Score
  # #         BstPara = para
  # BstPara = SVMParam[rowID-21]
  # # clf = SVR(epsilon=BstPara['epsilon'],kernel=BstPara['kernel'],C=BstPara['C'],gamma=BstPara['gamma'])
  # clf = NuSVR(nu=BstPara['nu'],kernel=BstPara['kernel'],C=BstPara['C'],gamma=BstPara['gamma'])
  # clf.fit(X_train_minmax,Y)
  # PredY0 = clf.predict(X_train_minmax)
  # PredY = clf.predict(X_test_minmax)

  # NN
  # from sklearn.neighbors import KNeighborsRegressor
  # parameters = list(ParameterGrid({'n_neighbors': range(50,800,50), 'weights': ('uniform', 'distance')}))
  # k_fold = cross_validation.KFold(n=len(Y), n_folds=5,random_state=4)
  # BstPara = None
  # BstScore = 1e5
  # for para in parameters:
  #   Score = 0
  #   for train_indices, test_indices in k_fold:
  #     TrX = X.ix[train_indices,]; TrY = Y[train_indices]
  #     TeX = X.ix[test_indices,]; TeY = Y[test_indices]
  #     clf1 = KNeighborsRegressor(n_neighbors=para['n_neighbors'], weights=para['weights'])
  #     clf1.fit(TrX,TrY)
  #     PeY = clf1.predict(TeX)
  #     Score += np.sqrt(mean_squared_error(TeY,PeY))
  #   if Score < BstScore:
  #     BstScore = Score
  #     BstPara = para
  # # BstPara = SVMParam[rowID-21]
  # clf = KNeighborsRegressor(n_neighbors=BstPara['n_neighbors'], weights=BstPara['weights'])
  # clf.fit(X,Y)
  # PredY0 = clf.predict(X)
  # PredY = clf.predict(AllNormVarTest)

  # # invest in top 1/3
  # Weight.ix[rowID-21,] = PredY
  # LThresh = np.percentile(PredY, 33)
  # HThresh = np.percentile(PredY, 67)
  # Weight.ix[rowID-21,PredY <= HThresh] = 0
  # clmID = np.isnan(Weight.ix[rowID - 21,])
  # Weight.ix[rowID - 21,clmID] = 0
  # # no short sell
  # clmID = Weight.ix[rowID-21,]<0
  # Weight.ix[rowID - 21,clmID] = 0

  # only invest in best stock
  Weight.ix[rowID-21,] = 0
  Index = PredY.tolist().index(np.nanmax(PredY))
  Weight.ix[rowID-21,Index] = 1

  #  #### classification of return > 10%
  Y1Mat = np.zeros(Target.ix[0:rowID,].shape)
  for ITER in range(0,rowID):
    # HThresh = np.percentile(Target.ix[ITER,], 90)
    # HThresh = np.percentile(Target.ix[ITER,], 95)
    # Y1Mat[ITER,Target.ix[ITER,]>=HThresh] = 1
    Y1Mat[ITER,] = ss.rankdata(Target.ix[ITER,],method='min')
  Y1 = Y1Mat.flatten()
  from sklearn import preprocessing
  min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))
  X_train_minmax = min_max_scaler.fit_transform(X)
  X_test_minmax = min_max_scaler.transform(AllNormVarTest)
  # parameters = list(ParameterGrid({'nu': np.arange(0.001,0.08,0.001), 'kernel': ('rbf', 'linear','sigmoid'),
  #                                  'gamma': [10**ii for ii in np.arange(-3,0,0.1)]}))
  parameters = list(ParameterGrid({'nu': np.arange(0.05,0.95,0.05), 'kernel': ('rbf', 'linear','sigmoid'),
                                   'gamma': [10**ii for ii in np.arange(-3,0,0.1)]}))
  # parameters = list(ParameterGrid({'epsilon': [10**ii for ii in range(-4,4)], 'kernel': ('rbf', 'linear','sigmoid'),
  #                                  'C': [10**ii for ii in range(-4,4)], 'gamma': [10**ii for ii in range(-4,4)]}))
  k_fold = cross_validation.StratifiedKFold(Y1, n_folds=5, random_state=4)
  BstPara = None
  BstScore = 1e5
  try:
    for para in parameters:
        Score = 0
        for train_indices, test_indices in k_fold:
            TrX = X_train_minmax[train_indices,]; TrY = Y1[train_indices]
            TeX = X_train_minmax[test_indices,]; TeY = Y1[test_indices]
            # clf1 = SVR(epsilon=para['epsilon'],kernel=para['kernel'],C=para['C'],gamma=para['gamma'])
            clf1 = NuSVC(nu=para['nu'],kernel=para['kernel'],probability=True,gamma=para['gamma'],cache_size=8000)
            weight = np.ones(TrY.shape)*sum(TrY)/len(TrY); weight[TrY==1]=(len(TrY)-sum(TrY))/len(TrY)
            # clf1.fit(TrX,TrY,sample_weight=weight)
            clf1.fit(TrX,TrY)
            PeY = clf1.predict_proba(TeX)
            Score += log_loss(TeY,PeY)
        if Score < BstScore:
            BstScore = Score
            BstPara = para
  except:
    print para
  # BstPara = SVMParam1[rowID-21]
  # clf = SVC(epsilon=BstPara['epsilon'],kernel=BstPara['kernel'],probability=True,gamma=BstPara['gamma'])
  clf = NuSVC(nu=BstPara['nu'],kernel=BstPara['kernel'],probability=True,gamma=BstPara['gamma'],cache_size=8000)
  weight = np.ones(Y1.shape)*sum(Y1); weight[Y1==1]=len(Y1)-sum(Y1)
  # clf.fit(X_train_minmax,Y1,sample_weight=weight)
  clf.fit(X_train_minmax,Y1)
  PredY0 = clf.predict_proba(X_train_minmax);PredY0=PredY0[:,-1]
  PredY = clf.predict_proba(X_test_minmax);PredY=PredY[:,-1]


  Weight.ix[rowID-21,] = 0
  # only invest in the best
  Index = PredY.tolist().index(np.nanmax(PredY))
  Weight.ix[rowID-21,Index] = 1
  # # invest in top 5
  # Weight.ix[rowID-21,] = 0
  # HThresh = np.percentile(PredY, 95)
  # Weight.ix[rowID-21,PredY >= HThresh] = 1
  # print sum(Weight.ix[rowID-21,])


  # #### ranking algorithm


  Weight.ix[rowID-21,Price.ix[rowID,].isnull()] = 0
  Adj = LevLimit*Capital.ix[rowID-21]/sum(abs(Price.ix[rowID,].fillna(0))*abs(Weight.ix[rowID-21,]))
  Weight.ix[rowID-21,] = Weight.ix[rowID-21,]*Adj
  PnL.ix[rowID-21] = np.nansum(Weight.ix[rowID-21,]*Target.ix[rowID,]*Price.ix[rowID,])
  if len(set(PredY)) < 2:
    PnL.ix[rowID-21] = 0
  Ret.ix[rowID-21] = PnL.ix[rowID-21]/Capital.ix[rowID-21,]
  Capital.ix[rowID-20] = Capital.ix[rowID-21] + PnL.ix[rowID-21]

  masked = ~np.isnan(YTest)&~np.isnan(PredY)
  masked1 = ~np.isnan(Y)&~np.isnan(PredY0)

  Pf2.append([ss.spearmanr(YTest[masked],PredY[masked])[0], ss.spearmanr(Y[masked1],PredY0[masked1])[0], Ret.ix[rowID-21]])
  print BstPara
  TMP = ss.rankdata(Target.ix[rowID,])
  print Weight.columns.values[Index], Target.ix[rowID,Index], TMP[Index]

print np.nanmean(Pf2,axis=0), np.nanstd(Pf2,axis=0)[-1]
print Pf2
print PnL
